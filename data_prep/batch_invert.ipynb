{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitch and Loudness Extraction from Wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchcrepe\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "from scipy.signal import butter, lfilter, filtfilt, resample\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, HubertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  wav files are sampled at 16 khz\n",
    "sr = 16000\n",
    "hop_length = int(sr / 200.) # EMA sampled at 200 Hz\n",
    "\n",
    "# This would be a reasonable range for speech\n",
    "fmin = 50\n",
    "fmax = 550\n",
    "\n",
    "# Select a model capacity--one of \"tiny\" or \"full\"\n",
    "model = 'tiny'\n",
    "\n",
    "# Choose a device to use for inference\n",
    "device = 'cuda:1'\n",
    "\n",
    "# Pick a batch size that doesn't cause memory errors on your gpu\n",
    "batch_size = 2048\n",
    "\n",
    "# EMA Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "speech_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\").eval().to(device)\n",
    "\n",
    "def extract_pitch(signal):\n",
    "    audio = torch.from_numpy(signal).unsqueeze(0).float()\n",
    "    \n",
    "    pitch, periodicity = torchcrepe.predict(audio,\n",
    "                            sr,\n",
    "                            hop_length,\n",
    "                            fmin,\n",
    "                            fmax,\n",
    "                            model,\n",
    "                            batch_size=batch_size,\n",
    "                            device=device,\n",
    "                            return_periodicity=True)\n",
    "    return pitch.flatten().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_loudness(signal):\n",
    "    signal = abs(signal)\n",
    "    loudness = np.array([max(signal[i:i+hop_length]) for i in range(0, signal.size, hop_length)])\n",
    "    return loudness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames_without_extension(directory):\n",
    "    return set(os.path.splitext(file)[0] for file in os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder to prep\n",
    "data_folder_name = _\n",
    "output_folder_name = _\n",
    "\n",
    "# Define the directories\n",
    "wav_dir = os.path.join(data_folder_name, \"wav\")\n",
    "loudness_dir = os.path.join(output_folder_name, \"loudness\")\n",
    "pitch_dir = os.path.join(output_folder_name, \"pitch_tiny\")\n",
    "ema_dir_input = os.path.join(data_folder_name, \"nema_npy\")\n",
    "ema_dir_output = os.path.join(output_folder_name, \"nema_npy\")\n",
    "\n",
    "# Make sure the output directories exist\n",
    "os.makedirs(loudness_dir, exist_ok=True)\n",
    "os.makedirs(pitch_dir, exist_ok=True)\n",
    "os.makedirs(ema_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Process each wav file\n",
    "for file in get_filenames_without_extension(ema_dir_input):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(os.path.join(wav_dir, file+'.wav'), sr=None)\n",
    "\n",
    "    # Extract the loudness and pitch\n",
    "    l = extract_loudness(y)\n",
    "    p = extract_pitch(y)\n",
    "\n",
    "    # Save the loudness and pitch to their own directories\n",
    "    # We use numpy's save function to save the arrays to .npy files\n",
    "    npy_file = file + '.npy'\n",
    "    np.save(os.path.join(loudness_dir, npy_file), l)\n",
    "    np.save(os.path.join(pitch_dir, npy_file), p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddsp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
